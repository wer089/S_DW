{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import DetectorFactory\n",
    "import string\n",
    "import en_core_sci_lg\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "import faiss\n",
    "import seaborn as sns\n",
    "from txtai.pipeline.data import tokenizer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/raw.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set seed\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# hold label - language\n",
    "languages = []\n",
    "\n",
    "# go through each text\n",
    "for ii in tqdm(range(0,len(df))):\n",
    "    # split by space into list, take the first x intex, join with space\n",
    "    text = df.iloc[ii]['body_text'].split(\" \")\n",
    "\n",
    "    lang = \"en\"\n",
    "    try:\n",
    "        if len(text) > 50:\n",
    "            lang = detect(\" \".join(text[:50]))\n",
    "        elif len(text) > 0:\n",
    "            lang = detect(\" \".join(text[:len(text)]))\n",
    "    # ught... beginning of the document was not in a good format\n",
    "    except Exception as e:\n",
    "        all_words = set(text)\n",
    "        try:\n",
    "            lang = detect(\" \".join(all_words))\n",
    "        # what!! :( let's see if we can find any text in abstract...\n",
    "        except Exception as e:\n",
    "\n",
    "            try:\n",
    "                # let's try to label it through the abstract then\n",
    "                lang = detect(df.iloc[ii]['abstract_summary'])\n",
    "            except Exception as e:\n",
    "                lang = \"unknown\"\n",
    "                pass\n",
    "\n",
    "    # get the language\n",
    "    languages.append(lang)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "languages_dict = {}\n",
    "for lang in set(languages):\n",
    "    languages_dict[lang] = languages.count(lang)\n",
    "\n",
    "print(\"Total: {}\\n\".format(len(languages)))\n",
    "print(languages_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['language'] = languages\n",
    "plt.bar(range(len(languages_dict)), list(languages_dict.values()), align='center')\n",
    "plt.xticks(range(len(languages_dict)), list(languages_dict.keys()))\n",
    "plt.title(\"Distribution of Languages in Dataset\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en = df[df['language'] == 'en']\n",
    "df_en = df_en.drop(['Unnamed: 0'],axis=1)\n",
    "df_en.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_en.to_csv('../Data/eng_only.csv')\n",
    "df_en = pd.read_csv('../Data/eng_only.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_abstract_only = df_en[df_en['abstract_summary'] != \"Not provided.\"]\n",
    "df_en_abstract_only = df_en_abstract_only[df_en_abstract_only['abstract'].notnull()]\n",
    "df_en_abstract_only.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_stop_words = [\n",
    "    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure',\n",
    "    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.',\n",
    "    'al.', 'Elsevier', 'PMC', 'CZI'\n",
    "]\n",
    "\n",
    "for stop in custom_stop_words:\n",
    "    if stop not in stopwords:\n",
    "        stopwords.add(stop)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('sentencizer')\n",
    "punctuation = string.punctuation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lemmatize_pipe(doc):\n",
    "    lemma_list = [str(tok.lemma_).lower().strip(punctuation) for tok in doc\n",
    "                  if tok.is_alpha and tok.text.lower() not in stopwords]\n",
    "    return lemma_list\n",
    "\n",
    "def preprocess_pipe(texts):\n",
    "    preproc_pipe = []\n",
    "    i=1\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=2)):\n",
    "        preproc_pipe.append(lemmatize_pipe(doc))\n",
    "        print('finished handling doc NO.{}'.format(i))\n",
    "        i+=1\n",
    "    return preproc_pipe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "df_en_abstract_only['self_processor'] = preprocess_pipe(df_en_abstract_only['abstract'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Parser\n",
    "punctuations = string.punctuation\n",
    "parser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\n",
    "parser.max_length = 7000000\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n",
    "    mytokens = \" \".join([i for i in mytokens])\n",
    "    return mytokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "df_en_abstract_only[\"article_processor\"] = df_en_abstract_only[\"abstract\"].progress_apply(spacy_tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_abstract_only.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_en_abstract_only = df_en_abstract_only.drop(['Unnamed: 0'],axis=1)\n",
    "# df_en_abstract_only.to_csv('../Data/article_proc.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Do Not Run Anything before this point!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_abstract_only = pd.read_csv('../Data/article_proc.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def vectorize(text, maxx_features):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=maxx_features).fit(text)\n",
    "    X = vectorizer.transform(text)\n",
    "    return X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "abstract = df_en_abstract_only['abstract'].values\n",
    "max_features = 2**12\n",
    "\n",
    "X = vectorize(abstract, max_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X[0].shape)\n",
    "print(X[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import KMeans"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_32 = X.todense().astype('float32')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_32 = X_32.toarray()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# run kmeans with many different k\n",
    "distortions = []\n",
    "K = range(2, 30)\n",
    "for k in K:\n",
    "    print('fitting clusters with {} clusters'.format(k))\n",
    "    k_means = KMeans(n_clusters=k, random_state=0,verbose=1,init='k-means++',max_iter=100).fit(X_32)\n",
    "    k_means.fit(X_32)\n",
    "    distortions.append(sum(np.min(cdist(X_32, k_means.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "    print('Found distortion for {} clusters'.format(k))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_init = 5\n",
    "max_iter = 100\n",
    "distortions = []\n",
    "K = range(10, 50)\n",
    "for k in K:\n",
    "    print('fitting clusters with {} clusters'.format(k))\n",
    "    k_means = faiss.Kmeans(d=X_32.shape[1], k=k, niter=max_iter, nredo=n_init,gpu=True,verbose=True,seed=553602)\n",
    "    k_means.train(X_32)\n",
    "    distortions.append(sum(np.min(cdist(X_32, k_means.centroids, 'euclidean'), axis=1)) / X.shape[0])\n",
    "    print('Found distortion for {} clusters'.format(k))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_line = [K[0], K[-1]]\n",
    "Y_line = [distortions[0], distortions[-1]]\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'b-')\n",
    "plt.plot(X_line, Y_line, 'r')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k = 40\n",
    "k_means = faiss.Kmeans(d=X_32.shape[1], k=k, niter=max_iter, nredo=n_init,gpu=True,verbose=True,seed=553602)\n",
    "k_means.train(X_32)\n",
    "label = k_means.assign(X_32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_abstract_only['cluster'] = label[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(verbose=1, perplexity=50)  # Changed perplexity from 100 to 50 per FAQ\n",
    "X_embedded = tsne.fit_transform(X_32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.color_palette(\"bright\", 1)\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\n",
    "plt.title('t-SNE with no Labels')\n",
    "plt.savefig(\"t-sne_covid19.png\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(20,20)})\n",
    "\n",
    "# colors\n",
    "palette = sns.hls_palette(40, l=.4, s=.9)\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=label[1], legend='full', palette=palette)\n",
    "plt.title('t-SNE with Kmeans Labels')\n",
    "plt.savefig(\"improved_cluster_tsne.png\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X_embedded[:,0].shape)\n",
    "print(X_embedded[:,0].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "X_reduced = TruncatedSVD(n_components=50, random_state=553602).fit_transform(X_32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k = 40\n",
    "n_init = 5\n",
    "max_iter = 100\n",
    "distortions = []\n",
    "k_means = faiss.Kmeans(d=X_reduced.shape[1], k=k, niter=max_iter, nredo=n_init,gpu=True,verbose=True,seed=553602)\n",
    "k_means.train(X_reduced)\n",
    "label_reduced = k_means.assign(X_reduced)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label_reduced"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne_reduced = TSNE(verbose=1, perplexity=50)  # Changed perplexity from 100 to 50 per FAQ\n",
    "X_embedded_reduced = tsne_reduced.fit_transform(X_reduced)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(20,20)})\n",
    "\n",
    "# colors\n",
    "palette = sns.hls_palette(40, l=.4, s=.9)\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded_reduced[:,0], X_embedded_reduced[:,1], hue=label_reduced[1], legend='full', palette=palette)\n",
    "plt.title('t-SNE with Kmeans Labels')\n",
    "plt.savefig(\"improved_cluster_tsne.png\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_abstract_only['cluster'] = label_reduced[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "abstract = df_en_abstract_only['abstract']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(abstract)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vectorizers = []\n",
    "\n",
    "for ii in range(0, 40):\n",
    "    # Creating a vectorizer\n",
    "    vectorizers.append(CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vectorized_data = []\n",
    "\n",
    "for current_cluster, cvec in tqdm(enumerate(vectorizers)):\n",
    "    try:\n",
    "        vectorized_data.append(cvec.fit_transform(df_en_abstract_only.loc[df_en_abstract_only['cluster'] == current_cluster, 'abstract']))\n",
    "    except Exception as e:\n",
    "        print(\"Not enough instances in cluster: \" + str(current_cluster))\n",
    "        vectorized_data.append(None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# number of topics per cluster\n",
    "NUM_TOPICS_PER_CLUSTER = 10\n",
    "\n",
    "\n",
    "lda_models = []\n",
    "\n",
    "for ii in range(0, 40):\n",
    "    # Latent Dirichlet Allocation Model\n",
    "    lda = LatentDirichletAllocation(n_components=NUM_TOPICS_PER_CLUSTER, max_iter=10, learning_method='online',verbose=True, random_state=42)\n",
    "    lda_models.append(lda)\n",
    "\n",
    "lda_models[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clusters_lda_data = []\n",
    "\n",
    "for current_cluster, lda in enumerate(lda_models):\n",
    "    print(\"Current Cluster: \" + str(current_cluster))\n",
    "\n",
    "    if vectorized_data[current_cluster] is not None:\n",
    "        clusters_lda_data.append((lda.fit_transform(vectorized_data[current_cluster])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Functions for printing keywords for each topic\n",
    "def selected_topics(model, vectorizer, top_n=5):\n",
    "    current_words = []\n",
    "    keywords = []\n",
    "\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        words = [(vectorizer.get_feature_names()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        for word in words:\n",
    "            if word[0] not in current_words:\n",
    "                keywords.append(word)\n",
    "                current_words.append(word[0])\n",
    "\n",
    "    keywords.sort(key = lambda x: x[1])\n",
    "    keywords.reverse()\n",
    "    return_values = []\n",
    "    for ii in keywords:\n",
    "        return_values.append(ii[0])\n",
    "    return return_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_keywords = []\n",
    "for current_vectorizer, lda in enumerate(lda_models):\n",
    "    print(\"Current Cluster: \" + str(current_vectorizer))\n",
    "\n",
    "    if vectorized_data[current_vectorizer] is not None:\n",
    "        all_keywords.append(selected_topics(lda, vectorizers[current_vectorizer]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f=open('../Data/topics.txt','w')\n",
    "\n",
    "count = 0\n",
    "\n",
    "for ii in all_keywords:\n",
    "\n",
    "    if vectorized_data[count] != None:\n",
    "        f.write(', '.join(ii) + \"\\n\")\n",
    "    else:\n",
    "        f.write(\"Not enough instances to be determined. \\n\")\n",
    "        f.write(', '.join(ii) + \"\\n\")\n",
    "    count += 1\n",
    "\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_abstract_only['cluster'] = label_reduced[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the COVID-19 DataFrame\n",
    "pickle.dump(df_en_abstract_only, open(\"../Data/df_en_abstract_only.p\", \"wb\" ))\n",
    "\n",
    "# save the final t-SNE\n",
    "pickle.dump(X_embedded, open(\"../Data/X_embedded_reduced.p\", \"wb\" ))\n",
    "\n",
    "# save the labels generate with k-means(20)\n",
    "pickle.dump(label, open(\"../Data/y_pred.p\", \"wb\" ))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# required libraries for plot\n",
    "from plot_text import header\n",
    "from call_backs import input_callback, selected_code\n",
    "import bokeh\n",
    "from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper\n",
    "from bokeh.palettes import viridis, plasma, Category20\n",
    "from bokeh.transform import linear_cmap, transform\n",
    "from bokeh.io import output_file, show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import RadioButtonGroup, TextInput, Div, Paragraph\n",
    "from bokeh.layouts import column, widgetbox, row, layout\n",
    "from bokeh.layouts import column\n",
    "from bokeh.io import curdoc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "topic_path = os.path.join(os.getcwd(), '../Data/topics.txt')\n",
    "with open(topic_path) as f:\n",
    "    topics = f.readlines()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_abstract_only['x'] = X_embedded_reduced[:,0]\n",
    "df_en_abstract_only['y'] = X_embedded_reduced[:,1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_bokeh_plot = []\n",
    "for i in range(40):\n",
    "    df_bokeh_plot.append(df_en_abstract_only.loc[df_en_abstract_only['cluster']==i].drop(['Unnamed: 0'],axis=1))\n",
    "    print(i)\n",
    "df_bokeh_plot[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show on notebook\n",
    "output_notebook()\n",
    "# target labels\n",
    "y_labels = label_reduced[1]\n",
    "\n",
    "# data sources\n",
    "\n",
    "\n",
    "# hover over information\n",
    "hover = HoverTool(tooltips=[\n",
    "    (\"Title\", \"@titles{safe}\"),\n",
    "    (\"Author(s)\", \"@authors{safe}\"),\n",
    "    (\"Journal\", \"@journal\"),\n",
    "    (\"Abstract\", \"@abstract{safe}\"),\n",
    "    (\"Link\", \"@links\")\n",
    "],\n",
    "point_policy=\"follow_mouse\")\n",
    "\n",
    "# map colors\n",
    "colors1 = plasma(10)\n",
    "colors2 = viridis(10)\n",
    "colors3 = Category20[20]\n",
    "colors = colors1+colors2+colors3\n",
    "# prepare the figure\n",
    "plot = figure(plot_width=1200, plot_height=850,\n",
    "           tools=[hover],\n",
    "           title=\"Clustering of the COVID-19 Literature with t-SNE and K-Means\")\n",
    "\n",
    "curdoc().theme = 'caliber'\n",
    "\n",
    "for cluster_data, name, color in zip(df_bokeh_plot, [i for i in range(40)], colors):\n",
    "\n",
    "    source = ColumnDataSource(data=dict(\n",
    "    x= cluster_data['x'],\n",
    "    y= cluster_data['y'],\n",
    "    desc= cluster_data['cluster'],\n",
    "    titles= cluster_data['title'],\n",
    "    authors = cluster_data['authors'],\n",
    "    journal = cluster_data['journal'],\n",
    "    abstract = cluster_data['abstract_summary'],\n",
    "    labels = [\"C-\" + str(x) for x in cluster_data['cluster']],\n",
    "    links = cluster_data['doi']\n",
    "    ))\n",
    "\n",
    "    plot.scatter('x', 'y', size=5,\n",
    "          fill_color=color,\n",
    "          fill_alpha=0.6,\n",
    "          line_alpha=0.3,\n",
    "          legend_label=str(name),\n",
    "          source=source)\n",
    "\n",
    "\n",
    "plot.legend.click_policy=\"hide\"\n",
    "plot.legend.background_fill_alpha = 0.6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# STYLE\n",
    "header.sizing_mode = \"stretch_width\"\n",
    "header.style={'color': '#2e484c', 'font-family': 'Julius Sans One, sans-serif;'}\n",
    "header.margin=5\n",
    "\n",
    "plot.sizing_mode = \"scale_both\"\n",
    "plot.margin = 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LAYOUT OF THE PAGE\n",
    "l = layout([\n",
    "    [header],\n",
    "    [plot]\n",
    "])\n",
    "l.sizing_mode = \"scale_both\"\n",
    "\n",
    "\n",
    "# show\n",
    "output_file('t-sne_covid-19_kmeans.html')\n",
    "show(l)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
