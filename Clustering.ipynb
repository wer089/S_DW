{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Clustering\n",
    "## Author: Zhenggang Tan\n",
    "- The goal of this notebook is to try out different clustering methods on the cleaned data.\n",
    "- Below is an example of:\n",
    "1. Vectorization of each article(abstract) to Word embedding vectors using Term Frequency and Inverse Document Frequency(TF-IDF).\n",
    "2. Clustering using K-means.\n",
    "3. Dimension reduction using t-distributed Stochastic Neighbor Embedding(t-SNE).\n",
    "- And finally make a plot combining cluster labels and dimension-reduced document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "import faiss\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Read in Data\n",
    "- The data will be too large to upload to GitHub. Please make sure you have the right directory set for the `read_csv` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_en_abstract_only = pd.read_csv('../Data/article_proc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_en_abstract_only.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define vectorization technique\n",
    "- Here we used TF-IDF as Vectorization technique.\n",
    "\n",
    "TODO: TF-IDF short explanation\n",
    "\n",
    "Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. \n",
    "\n",
    "Term Frequency, which measures how frequently a term occurs in a document.\n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones.\n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- We hope eventually we could try out different vectorization techniques and find which one is most suitable for this task.\n",
    "TODO: implement `Word2Vec`, `FastText`, `BERT` version of word embedding technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize(text, maxx_features):\n",
    "    vectorizer = TfidfVectorizer(max_features=maxx_features).fit(text)\n",
    "    X = vectorizer.transform(text)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use abstract for embedding otherwise will take too long\n",
    "abstract = df_en_abstract_only['abstract'].values\n",
    "# Set maximum for number of features we want.\n",
    "# Although the more feature the richer the content\n",
    "# You don't want it to be too large because our\n",
    "# Dataset has around 240000 samples, and later when\n",
    "# We want to fit clustering algorithms we have to\n",
    "# Turn the word embeddings to numpy array(either np.float64\n",
    "# or np.float32) which requires 240000*max_features*4 bytes in\n",
    "# Memory. 4096 feature requires about 3.66 GB on RAM! Consider\n",
    "# Changing this value to a smaller one if you run into memory error.\n",
    "max_features = 2**12\n",
    "\n",
    "X = vectorize(abstract, max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Clustering\n",
    "- Below is our demo clustering using K-means.\n",
    "Selected `K=40` according to distortions of `K` from 10 to 50(Could try more).\n",
    "- TODO: Implement with more clustering techniques, possible ones are:\n",
    "1. Spectral clustering\n",
    "2. DBSCAN(might not work well since it's heavily used for outlier detection).\n",
    "3. Gaussian Mixture Model\n",
    "4. BIRCH\n",
    "5. Affinity Propagation clustering\n",
    "6. Mean-Shift clustering\n",
    "7. OPTICS algorithm\n",
    "8. Agglomerative Hierarchy clustering\n",
    "See this link for explanations and code: [Clustering](https://www.freecodecamp.org/news/8-clustering-algorithms-in-machine-learning-that-all-data-scientists-should-know/)\n",
    "Ideally, we should select the same number of clusters for each algorithm, so we could compare them on the same scale.\n",
    "<strong>Caution!</strong>: There are many packages out there that might out-perform `scikit-learn` on a specific task(i.e. Facebook's `Faiss` for K-means and PCA). Considering the scope of our dataset, try what's the best to save some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_32 = X.todense().astype('float32').toarray()\n",
    "n_init = 5\n",
    "max_iter = 100\n",
    "distortions = []\n",
    "\n",
    "# run kmeans with many different k\n",
    "K = range(10, 50)\n",
    "for k in K:\n",
    "    print('fitting clusters with {} clusters'.format(k))\n",
    "    k_means = faiss.Kmeans(d=X_32.shape[1], k=k, niter=max_iter, nredo=n_init,gpu=True,verbose=True,seed=553602)\n",
    "    k_means.train(X_32)\n",
    "    distortions.append(sum(np.min(cdist(X_32, k_means.centroids, 'euclidean'), axis=1)) / X.shape[0])\n",
    "    print('Found distortion for {} clusters'.format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot to choose a best K\n",
    "X_line = [K[0], K[-1]]\n",
    "Y_line = [distortions[0], distortions[-1]]\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'b-')\n",
    "plt.plot(X_line, Y_line, 'r')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Chose 40 as best K, rerun with K=40 and predict label for each abstract(on the word embeddings).\n",
    "k = 40\n",
    "k_means = faiss.Kmeans(d=X_32.shape[1], k=k, niter=max_iter, nredo=n_init,gpu=True,verbose=True,seed=553602)\n",
    "k_means.train(X_32)\n",
    "label = k_means.assign(X_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assign clusters to dataset.\n",
    "# CHANGE THE VARIABLE NAME TO SOMETHING ELSE! DO NOT OVERWRITE!\n",
    "df_en_abstract_only['cluster'] = label[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Dimension Reduction for plotting\n",
    "- Our goal is to have some nice visualization of articles that are similar to be plotted together somehow.\n",
    "- How? Dimension reduction! Specifically, we reduce the dataset's feature number to 2(2D) or 3(3D), so we can construct visualizations.\n",
    "- TODO: t-SNE explanation\n",
    "See this link for paper link and implementation: [t-SNE](https://lvdmaaten.github.io/tsne/)\n",
    "- There could be other techniques that do this but this is the best algorithm so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(verbose=1, perplexity=50)  # Changed perplexity from 100 to 50 per FAQ\n",
    "X_embedded = tsne.fit_transform(X_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualization without coloring using labels\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.color_palette(\"bright\", 1)\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\n",
    "plt.title('t-SNE with no Labels')\n",
    "plt.savefig(\"t-sne_covid19.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualization using labels\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(20,20)})\n",
    "\n",
    "# colors\n",
    "palette = sns.hls_palette(40, l=.4, s=.9)\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=label[1], legend='full', palette=palette)\n",
    "plt.title('t-SNE with Kmeans Labels')\n",
    "plt.savefig(\"improved_cluster_tsne.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### <strong>OPTIONAL BUT HIGHLY RECOMMENDED</strong>\n",
    "- You have seen our visualization and probably find it to be an awful clustering.\n",
    "- Why? We used all of our features to cluster, but not all of them are useful. In fact, most of them would be useless if we think in a way of Principal Component Analysis. However, our word embedding matrix is a sparse matrix which does not work well with PCA, so we have to use `Truncated SVD` to truncate our features to `n` dimensions that explains the most variances. From there, we fit our clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_reduced = TruncatedSVD(n_components=50, random_state=553602).fit_transform(X_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 40\n",
    "k_means = faiss.Kmeans(d=X_reduced.shape[1], k=k, niter=max_iter, nredo=n_init,gpu=True,verbose=True,seed=553602)\n",
    "k_means.train(X_reduced)\n",
    "label_reduced = k_means.assign(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne_reduced = TSNE(verbose=1, perplexity=50)  # Changed perplexity from 100 to 50 per FAQ\n",
    "X_embedded_reduced = tsne_reduced.fit_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(20,20)})\n",
    "\n",
    "# colors\n",
    "palette = sns.hls_palette(40, l=.4, s=.9)\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded_reduced[:,0], X_embedded_reduced[:,1], hue=label_reduced[1], legend='full', palette=palette)\n",
    "plt.title('t-SNE with Kmeans Labels')\n",
    "plt.savefig(\"improved_cluster_tsne.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This plot now makes much more sense. Later we will perform topic modeling on each of the clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
